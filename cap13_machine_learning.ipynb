{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Python para Ciência de Dados</font>\n",
    "# <font color='blue'>Capítulo 13</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python usada neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão Linear Simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Dados de exemplo\n",
    "X = [[1], [2], [3], [4], [5]]\n",
    "y = [2, 4, 5, 4, 5]\n",
    "\n",
    "# Criar e treinar o modelo\n",
    "modelo = LinearRegression()\n",
    "modelo.fit(X, y)\n",
    "\n",
    "# Prever novos valores\n",
    "novos_valores = [[6], [7]]\n",
    "predicoes = modelo.predict(novos_valores)\n",
    "print(predicoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão Linear Múltipla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de exemplo\n",
    "X = [[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]]\n",
    "y = [2, 4, 5, 4, 5]\n",
    "\n",
    "# Criar e treinar o modelo\n",
    "modelo = LinearRegression()\n",
    "modelo.fit(X, y)\n",
    "\n",
    "# Prever novos valores\n",
    "novos_valores = [[6, 7], [7, 8]]\n",
    "predicoes = modelo.predict(novos_valores)\n",
    "print(predicoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Dados de exemplo\n",
    "X = [[1], [2], [3], [4], [5]]\n",
    "y = [2, 4, 5, 4, 5]\n",
    "\n",
    "# Criar e treinar o modelo\n",
    "modelo = Ridge(alpha=0.1) # Alpha é o parâmetro de regularização\n",
    "modelo.fit(X, y)\n",
    "\n",
    "# Prever novos valores\n",
    "novos_valores = [[6], [7]]\n",
    "predicoes = modelo.predict(novos_valores)\n",
    "print(predicoes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Dados de exemplo\n",
    "X = [[1], [2], [3], [4], [5]]\n",
    "y = [2, 4, 5, 4, 5]\n",
    "\n",
    "# Criar e treinar o modelo\n",
    "modelo = Lasso(alpha=0.1) # Alpha é o parâmetro de regularização\n",
    "modelo.fit(X, y)\n",
    "\n",
    "# Prever novos valores\n",
    "novos_valores = [[6], [7]]\n",
    "predicoes = modelo.predict(novos_valores)\n",
    "print(predicoes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Carregar conjunto de dados de exemplo\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Dividir dados em conjunto de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Criar e treinar o modelo\n",
    "modelo = GaussianNB()\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "predicoes = modelo.predict(X_test)\n",
    "\n",
    "# Avaliar a precisão do modelo\n",
    "precisao = accuracy_score(y_test, predicoes)\n",
    "print(\"Precisão:\", precisao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Árvores de Decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Criar e treinar o modelo\n",
    "modelo = DecisionTreeClassifier()\n",
    "\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "predicoes = modelo.predict(X_test)\n",
    "\n",
    "# Avaliar a precisão do modelo\n",
    "precisao = accuracy_score(y_test, predicoes)\n",
    "print(\"Precisão:\", precisao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Criar e treinar o modelo\n",
    "modelo = RandomForestClassifier()\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "predicoes = modelo.predict(X_test)\n",
    "\n",
    "# Avaliar a precisão do modelo\n",
    "precisao = accuracy_score(y_test, predicoes)\n",
    "print(\"Precisão:\", precisao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Carregar o conjunto de dados\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Dividir o conjunto de dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criar e treinar o modelo Gradient Boosting\n",
    "modelo = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "previsoes = modelo.predict(X_test)\n",
    "\n",
    "# Calcular a precisão do modelo\n",
    "precisao = accuracy_score(y_test, previsoes)\n",
    "print(\"Precisão:\", precisao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (Extreme Gradient Boosting) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalando a biblioteca xgboost\n",
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Carregar o conjunto de dados\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Dividir o conjunto de dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criar o objeto DMatrix (necessário para o XGBoost)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Definir os parâmetros do modelo\n",
    "parametros = {\n",
    " 'max_depth': 3, # Profundidade máxima da árvore\n",
    " 'eta': 0.3, # Taxa de aprendizado\n",
    " 'objective': 'multi:softmax', # Função de perda para problemas de classificação multiclasse\n",
    " 'num_class': 3 # Número de classes no conjunto de dados\n",
    "}\n",
    "\n",
    "# Treinar o modelo\n",
    "num_round = 100\n",
    "modelo = xgb.train(parametros, dtrain, num_round)\n",
    "\n",
    "# Fazer previsões\n",
    "previsoes = modelo.predict(dtest)\n",
    "\n",
    "# Calcular a precisão do modelo\n",
    "precisao = accuracy_score(y_test, previsoes)\n",
    "print(\"Precisão:\", precisao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Criar e treinar o modelo\n",
    "modelo = SVC()\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "predicoes = modelo.predict(X_test)\n",
    "\n",
    "# Avaliar a precisão do modelo\n",
    "precisao = accuracy_score(y_test, predicoes)\n",
    "print(\"Precisão:\", precisao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Carregar o conjunto de dados Iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Dividir os dados em conjunto de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criar e treinar o modelo KNN\n",
    "modelo = KNeighborsClassifier(n_neighbors=5)  # k=5 (por exemplo)\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "previsoes = modelo.predict(X_test)\n",
    "\n",
    "# Calcular a precisão do modelo\n",
    "precisao = accuracy_score(y_test, previsoes)\n",
    "print(\"Precisão:\", precisao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rede Neural Artificial (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Carregar o conjunto de dados Iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Dividir os dados em conjunto de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pré-processamento dos dados: normalização\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Criar e treinar o modelo de rede neural\n",
    "modelo = MLPClassifier(hidden_layer_sizes=(10, 5), activation='relu', max_iter=1000, random_state=42)\n",
    "modelo.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "previsoes = modelo.predict(X_test_scaled)\n",
    "\n",
    "# Calcular a precisão do modelo\n",
    "precisao = accuracy_score(y_test, previsoes)\n",
    "print(\"Precisão:\", precisao)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNs - Redes Neurais Convolucionais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalando a biblioteca tensorflow\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Carregar e preparar o conjunto de dados MNIST\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Definir a arquitetura da CNN\n",
    "modelo = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar o modelo\n",
    "modelo.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Treinar o modelo\n",
    "history = modelo.fit(train_images, train_labels, epochs=5, \n",
    "                    validation_data=(test_images, test_labels))\n",
    "\n",
    "# Avaliar o modelo\n",
    "test_loss, test_acc = modelo.evaluate(test_images,  test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "# Plotar a acurácia de treinamento e validação ao longo do tempo\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs - Redes Neurais Recorrentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "import numpy as np\n",
    "\n",
    "# Dados de exemplo\n",
    "# Suponhamos que temos uma sequência de números de 0 a 9\n",
    "# Queremos treinar a RNN para prever o próximo número na sequência\n",
    "dados = np.array([i for i in range(10)])\n",
    "dados = np.reshape(dados, (1, 10, 1))  # Formato de entrada para a RNN: (número de amostras, comprimento da sequência, número de recursos)\n",
    "\n",
    "# Definir modelo RNN\n",
    "modelo = Sequential([\n",
    "    layers.SimpleRNN(50, input_shape=(10, 1), return_sequences=True),  # Camada RNN com 50 unidades\n",
    "    layers.Dense(1)  # Camada densa para prever o próximo número\n",
    "])\n",
    "\n",
    "# Compilar o modelo\n",
    "modelo.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Treinar o modelo\n",
    "modelo.fit(dados, dados, epochs=100, verbose=1)\n",
    "\n",
    "# Testar o modelo\n",
    "previsoes = modelo.predict(dados)\n",
    "print(\"Previsões:\", previsoes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GANs -Redes Generativas Adversariais "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rede Generativa Adversarial (GAN) usando a biblioteca TensorFlow para gerar imagens semelhantes ao conjunto de dados CIFAR-10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de treinamento de uma GAN sobre o dataset MNIST\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import vstack\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Definindo o modelo discriminador\n",
    "def define_discriminator(in_shape=(28,28,1)):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=in_shape))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\tmodel.add(Dropout(0.4))\n",
    "\tmodel.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\tmodel.add(Dropout(0.4))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compilar o modelo\n",
    "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# Definindo o modelo gerador\n",
    "def define_generator(latent_dim):\n",
    "\tmodel = Sequential()\n",
    "\t# Base para imagem 7x7\n",
    "\tn_nodes = 128 * 7 * 7\n",
    "\tmodel.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\tmodel.add(Reshape((7, 7, 128)))\n",
    "\t# Aumentando a resolucao para 14x14\n",
    "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\t# Aumentando a resolucao para 28x28\n",
    "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\tmodel.add(Conv2D(1, (7,7), activation='sigmoid', padding='same'))\n",
    "\treturn model\n",
    "\n",
    "# Definindo o modelo combinado de gerador e discriminador, para atualizacao do gerador\n",
    "def define_gan(g_model, d_model):\n",
    "\t# tornar os pesos no discriminador não treináveis\n",
    "\td_model.trainable = False\n",
    "\t# conectar os modelos\n",
    "\tmodel = Sequential()\n",
    "\t# adicionar o gerador\n",
    "\tmodel.add(g_model)\n",
    "\t# adicionar o discriminador\n",
    "\tmodel.add(d_model)\n",
    "\t# compilar o modelo\n",
    "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\treturn model\n",
    "\n",
    "# Carregando e preparando as imagens de treino do dataset mnist\n",
    "def load_real_samples():\n",
    "\t# carregar o dataset mnist\n",
    "\t(trainX, _), (_, _) = load_data()\n",
    "\t# expandir para 3d, por ex. adicionar dimensão de canais\n",
    "\tX = expand_dims(trainX, axis=-1)\n",
    "\t# converter de ints não atribuidos para floats\n",
    "\tX = X.astype('float32')\n",
    "\t# escalar de [0,255] para [0,1]\n",
    "\tX = X / 255.0\n",
    "\treturn X\n",
    "\n",
    "# Selecionando amostras reais\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "\t# escolher instancias randomicamente\n",
    "\tix = randint(0, dataset.shape[0], n_samples)\n",
    "\t# recuperar imagens selecionadas\n",
    "\tX = dataset[ix]\n",
    "\t# gerar rótulos de classe 'real' (1)\n",
    "\ty = ones((n_samples, 1))\n",
    "\treturn X, y\n",
    "\n",
    "# Gerando pontos no espaço latente como entrada para o gerador\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "\t# gerar pontos no espaço latente\n",
    "\tx_input = randn(latent_dim * n_samples)\n",
    "\t# remodelar (reshape) em um batch de entradas para a rede\n",
    "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
    "\treturn x_input\n",
    "\n",
    "# Usando o gerador para gerar n exemplos falsos, com rótulos de classe\n",
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "\t# gerar pontos no espaço latente\n",
    "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
    "\t# prever resultados\n",
    "\tX = g_model.predict(x_input)\n",
    "\t# criar rótulos de classe 'fake' (0)\n",
    "\ty = zeros((n_samples, 1))\n",
    "\treturn X, y\n",
    "\n",
    "# Criando e salvando um gráfico de imagens geradas (tons de cinza invertidos)\n",
    "def save_plot(examples, epoch, n=10):\n",
    "\t# plotar imagens\n",
    "\tfor i in range(n * n):\n",
    "\t\t# definir subplot\n",
    "\t\tpyplot.subplot(n, n, 1 + i)\n",
    "\t\t# desligar axis\n",
    "\t\tpyplot.axis('off')\n",
    "\t\t# plotar dados brutos de pixel\n",
    "\t\tpyplot.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
    "\t# salvar plot em arquivo\n",
    "\tfilename = 'generated_plot_e%03d.png' % (epoch+1)\n",
    "\tpyplot.savefig(filename)\n",
    "\tpyplot.close()\n",
    "\n",
    "# Avaliando o discriminador, plotando as imagens geradas e salvando o modelo gerador\n",
    "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=100):\n",
    "\t# preparar amostras reais\n",
    "\tX_real, y_real = generate_real_samples(dataset, n_samples)\n",
    "\t# avaliar o discriminador em exemplos reais\n",
    "\t_, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
    "\t# preparar amostras falsas\n",
    "\tx_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "\t# avaliar o discriminador em exemplos falsos\n",
    "\t_, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "\t# resumir o desempenho do discriminador\n",
    "\tprint('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "\t# savar plot\n",
    "\tsave_plot(x_fake, epoch)\n",
    "\t# salve o arquivo de bloco do modelo do gerador\n",
    "\tfilename = 'generator_model_%03d.h5' % (epoch + 1)\n",
    "\tg_model.save(filename)\n",
    "\n",
    "# Treinando o gerador e o discriminador\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=256):\n",
    "\tbat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "\thalf_batch = int(n_batch / 2)\n",
    "\t# enumerar epochs manualmente\n",
    "\tfor i in range(n_epochs):\n",
    "\t\t# enumerar batches sobre o conjunto de treino\n",
    "\t\tfor j in range(bat_per_epo):\n",
    "\t\t\t# obter amostras 'reais' selecionadas aleatoriamente\n",
    "\t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "\t\t\t# gerar exemplos 'falsos'\n",
    "\t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "\t\t\t# criar conjunto de treinamento para o discriminador\n",
    "\t\t\tX, y = vstack((X_real, X_fake)), vstack((y_real, y_fake))\n",
    "\t\t\t# atualizar pesos do modelo discriminador\n",
    "\t\t\td_loss, _ = d_model.train_on_batch(X, y)\n",
    "\t\t\t# preparar pontos no espaco latente como entrada para o gerador\n",
    "\t\t\tX_gan = generate_latent_points(latent_dim, n_batch)\n",
    "\t\t\t# crie rotulos invertidos para as amostras falsas\n",
    "\t\t\ty_gan = ones((n_batch, 1))\n",
    "\t\t\t# atualize o gerador atraves dos erros do discriminador\n",
    "\t\t\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "\t\t\t# resumir a perda neste batch\n",
    "\t\t\tprint('>%d, %d/%d, d=%.3f, g=%.3f' % (i+1, j+1, bat_per_epo, d_loss, g_loss))\n",
    "\t\t# avaliar o desempenho do modelo, as vezes\n",
    "\t\tif (i+1) % 10 == 0:\n",
    "\t\t\tsummarize_performance(i, g_model, d_model, dataset, latent_dim)\n",
    "\n",
    "# Tamanho do espaço latente\n",
    "latent_dim = 100\n",
    "# Criando o discriminador\n",
    "d_model = define_discriminator()\n",
    "# Criando o gerador\n",
    "g_model = define_generator(latent_dim)\n",
    "# Criando a GAN\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "# Carregando os dados de imagem\n",
    "dataset = load_real_samples()\n",
    "# Treinando o modelo\n",
    "train(g_model, d_model, gan_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fim"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
